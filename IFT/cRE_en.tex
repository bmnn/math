% [p. 12]
% 'chain rule for entropy'
For a sequence of random variables $ \operatorname{X}^{n} = \operatorname{X}_{1}, \operatorname{X}_{2}, \dots \operatorname{X}_{n}$ we have for the joint entropy:
$$\operatorname{H}( \operatorname{X}_{1}, \operatorname{X}_{2}, \dots \operatorname{X}_{n} ) = \sum\limits_{i = 1}^{n} \operatorname{H}(\operatorname{X}_{i} | \operatorname{X}^{i - 1})$$
